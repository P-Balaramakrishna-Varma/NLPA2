{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conllu\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_file, vocab_index, pos_tag_index):\n",
    "    TokenLists = conllu.parse_incr(open(data_file, \"r\", encoding=\"utf-8\"))\n",
    "    Sentences = []\n",
    "    Tag_Sequences = []\n",
    "    for TokenList in TokenLists:\n",
    "        Sentence = []\n",
    "        tags = []\n",
    "        for token in TokenList:\n",
    "            #print(token[\"form\"], token[\"upos\"])\n",
    "            Sentence.append(vocab_index[token[\"form\"]])\n",
    "            tags.append(pos_tag_index[token[\"upos\"]])\n",
    "        Sentences.append(Sentence)\n",
    "        Tag_Sequences.append(tags)\n",
    "    return Sentences, Tag_Sequences\n",
    "\n",
    "\n",
    "def get_vocab_index(data_file):\n",
    "    vocab_index = {}\n",
    "    TokenLists = conllu.parse_incr(open(data_file, \"r\", encoding=\"utf-8\"))\n",
    "    for TokenList in TokenLists:\n",
    "        for token in TokenList:\n",
    "            if token[\"form\"] not in vocab_index:\n",
    "                vocab_index[token[\"form\"]] = len(vocab_index) + 1\n",
    "    return vocab_index\n",
    "\n",
    "\n",
    "def custom_collate(batch):\n",
    "    Sentences = [sample[0] for sample in batch]\n",
    "    PosTags = [sample[1] for sample in batch]\n",
    "\n",
    "    Sentences = pad_sequence(Sentences, batch_first=True)\n",
    "    PosTags = pad_sequence(PosTags, batch_first=True)\n",
    "    return Sentences, PosTags\n",
    "\n",
    "\n",
    "class PosTagDataset(Dataset):\n",
    "    def __init__(self, data_file):\n",
    "        self.vocab_index = get_vocab_index(data_file)\n",
    "        self.pos_tag_index = {\"ADJ\": 17, \"ADP\": 1, \"ADV\": 2, \"AUX\": 3, \"CCONJ\": 4, \"DET\": 5, \"INTJ\": 6, \"NOUN\": 7, \"NUM\": 8, \"PART\": 9, \"PRON\": 10, \"PROPN\": 11, \"PUNCT\": 12, \"SCONJ\": 13, \"SYM\": 14, \"VERB\": 15, \"X\": 16}\n",
    "        self.Sentences, self.Tag_Sequences = get_data(data_file, self.vocab_index, self.pos_tag_index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.LongTensor(self.Sentences[idx]), torch.LongTensor(self.Tag_Sequences[idx])\n",
    "    \n",
    "\n",
    "class PosTagModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, targe_size, embedding_dim, hidden_dim, no_layers):\n",
    "        # Embeding layer\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # BLSTM layer\n",
    "        self.blstm = torch.nn.LSTM(embedding_dim, hidden_dim, no_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Output layer (*2 because of bidirectional)\n",
    "        self.out_linear = torch.nn.Linear(hidden_dim * 2, targe_size)\n",
    "        self.out_activation = torch.nn.Relu()\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.embedding(X)\n",
    "        \n",
    "        X, _ = self.blstm(X)\n",
    "\n",
    "        X = self.out_linear(X)\n",
    "        X = self.out_activation(X)\n",
    "        return X\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"./UD_English-Atis/en_atis-ud-dev.conllu\"\n",
    "\n",
    "vocab_index = get_vocab_index(data_file)\n",
    "pos_tag_index = { \"ADJ\": 17, \"ADP\": 1, \"ADV\": 2, \"AUX\": 3, \"CCONJ\": 4, \"DET\": 5, \"INTJ\": 6, \"NOUN\": 7, \"NUM\": 8, \"PART\": 9, \"PRON\": 10, \"PROPN\": 11, \"PUNCT\": 12, \"SCONJ\": 13, \"SYM\": 14, \"VERB\": 15, \"X\": 16}\n",
    "\n",
    "Sentences, Tag_Sequences = get_data(data_file, vocab_index, pos_tag_index)\n",
    "\n",
    "\n",
    "for i in range(len(Sentences)):\n",
    "    for j in range(len(Sentences[i])):\n",
    "        print(Sentences[i][j], Tag_Sequences[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PosTagDataset(data_file)\n",
    "print(dataset[0])\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size=32, shuffle=False, collate_fn=custom_collate)\n",
    "for batch in train_dataloader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def train_loop(model, loss_fn, optimizer, train_dataloader):\n",
    "    size = len(train_dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        pred, _ = model(X)               \n",
    "        pred = pred.reshape(batch_size * len, -1)   \n",
    "        y = y.reshape(-1)\n",
    "        loss = loss_fn(pred, y)\n",
    "    \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPA2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "065fb8b657f91755f4fa62e8fa2cb5eae56030de00e55e0c25fcdec87b4196ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
